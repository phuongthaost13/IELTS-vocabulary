---
title: "Một số insight về từ vựng sách Cambridge English IELTS"
author: "Phạm Phương Thảo"
date: "8/30/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

```{r library, warning=FALSE, message=FALSE}
library(ggplot2)
library(hrbrthemes)
library(scales)
library(dplyr)
library(data.table)
library(tidytext)
library(purrr) 
#library(plyr) # k load package này bởi vì nó mask dplyr::group_by
```

```{r data-for-vis}
# data import
setwd("~/R project/textMining/ielts-vocab/output/cam10-15")
temp <- list.files(pattern = "*.csv")
myFiles <- lapply(temp, read.csv)


len_myFiles <- length(myFiles)
for (i in 1:len_myFiles) {
  myFiles[[i]]$source <- paste("Cambridge", i+9)
}

# combine all dataframe into one
combined_df <- bind_rows(myFiles)
len_cd <- nrow(combined_df)
for (i in 1:len_cd) {
  if (combined_df$word[i] %in% stop_words$word) {
  combined_df$word_type[i] = 'sw'
} else {
  combined_df$word_type[i] = 'nm'
}
}


# question sample words 
ques_sample <- read.csv(here::here('output', 'ques_sample.csv'))

```

# Giới thiệu qua về dữ liệu

Mình sử dụng R để thực hiện việc khai thác dữ liệu văn bản.

Cambridge English IELTS là bộ sách luyện thi IELTS chính thức tương tự như đề thi IELTS thật. 
Đây là sách có bản quyền và vì không có được nguồn sách hợp pháp nên mình phải sử dụng các file sách được chia sẻ lậu trên internet. 
Sách này được chia sẻ ở hai dạng: một là PDF chữ và hai là PDF dạng scan hình ảnh. 
Trong số 6 cuốn sách mình sử dụng (từ Cambridge English IELTS 10 - 15), chỉ duy nhất cuốn Cambridge 11 là ở dạng PDF chữ. 
Quá trình chuyển từ PDF chữ thành text để khai thác dữ liệu khá đơn giản và chính xác. 
Các cuốn sách còn lại đều phải đưa vào R để thực hiện việc chuyển file hình ảnh thành text, độ chính xác thấp hơn do chất lượng hình ảnh nhiều chỗ không tốt.

Phần dữ liệu được khai thác trong sách chỉ bao gồm đề thi của phần IELTS Academic, không bao gồm phần IELTS General, listening transcript và answer keys. 

# Mục đích thực hiện

Mục đích của mình khi thực hiện việc khai thác dữ liệu chữ từ sách nhằm lọc ra những từ vựng có khả năng xuất hiện cao nhất trong đề thi IELTS. 
Từ đó, giúp định hướng cho việc học từ vựng, bắt đầu từ những từ vựng thông dụng nhất.

# Khai thác dữ liệu văn bản

*Một số lưu ý:

- Trong bài viết, mình sẽ có sự phân biệt giữa từ vựng (vocabulary) và từ (word). 

- Trong R có tích hợp một tập dữ liệu những từ vựng thông dụng nhất trong tiếng Anh gọi là stopwords (bao gồm 1149 từ). 
Đây là những từ vựng bắt buộc phải học nên trong các phân tích của mình, từ vựng loại này đều đã được loại ra hoặc nếu có mình sẽ có chú thích riêng trong từng biểu đồ.

- Từ thời điểm này trở về sau, mình sẽ sử dụng từ "Cam" để viết tắt thay cho "Cambridge English IELTS".

```{r num-vocabs}
ggplot(combined_df, aes(x = source)) +
  
  geom_bar(fill = 'light blue', col = 'light blue', alpha = 0.5) +
  
  geom_text(
    stat = 'count',
    aes(label = ..count..),
    vjust = -0.5,
    family = 'serif',
    size = 2.7,
    color = ("dark red"),
    fontface = 'bold')+
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tổng số từ vựng (từ vựng không lặp lại)',
    x = 'Tên sách',
    y = 'Số lượng từ') +
  
  theme_minimal() +
  
  theme(#grid elements
      axis.ticks = element_blank(),          #strip axis ticks
      axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
      axis.title.y = element_text(hjust = 1, face = 'bold'))
  
```

Số lượng từ vựng xuất hiện trong sách Cam qua các năm xấp xỉ bằng nhau, riêng chỉ cuốn Cam 11 có lượng từ lớn hơn rất nhiều. 
Có hai lí do để giải thích. Một là như mình đã đề cập, cuốn Cam 11 được chuyển từ PDF chữ nên độ chính xác cao hơn. 
Lí do thứ hai đó là cuốn Cam 11 thực sự có lượng từ lớn hơn các cuốn khác.

```{r num-words}
combined_df %>% 
  group_by(source) %>% 
  summarise(n = sum(freq)) %>% 
  
ggplot(aes(x = source, y = n)) +
  
  geom_bar(fill = 'light blue', col = 'light blue', alpha = 0.5, stat = 'identity') +
  
  geom_text(
    stat = 'identity',
    aes(label = n),
    vjust = -0.5,
    family = 'serif',
    size = 2.7,
    color = ("dark red"),
    fontface = 'bold')+
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tổng số từ (từ vựng có lặp lại)',
    caption = 'Có bao gồm các từ thông dụng',
    x = 'Tên sách',
    y = 'Số lượng từ') +
  
  theme_minimal() +
  
  theme(
      axis.ticks = element_blank(),          #strip axis ticks
      axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
      axis.title.y = element_text(hjust = 1, face = 'bold'),
      plot.caption = element_text( hjust = 0))             #left align
      
```

Để kiểm định cho hai giả thuyết nêu trên, mình lập biểu đồ tổng số lượng từ xuất hiện trong sách. 
Bởi vì đề thi IELTS có thời lượng làm bài giống nhau nên mình suy luận rằng chúng cũng sẽ có lượng từ tương đương nhau. 
Nếu giả thuyết thứ nhất là đúng, thì tổng số từ của cuốn Cam 11 cũng phải cao hơn các cuốn Cam khác. 
Tuy nhiên, nhìn vào biểu đồ, số lượng từ của các cuốn đều xấp xỉ nhau nên mình tin rằng Cam 11 thực sự có lượng từ vựng nhiều hơn các cuốn khác chứ không phải do R đã thiếu chính xác trong việc xử lí file hình ảnh thành text.

```{r stopwords-normalwords}
ggplot(combined_df, aes(x = word_type, group = source)) +
  
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = 'count', alpha=0.5)+
  
  geom_text(
    #stat = 'freq',
    aes(label = scales::percent(..prop..),
        y = ..prop..), 
    stat = 'count',
    vjust = 1,
    family = 'serif',
    size = 2.7,
    color = 'black',
    fontface = 'bold') +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tỉ lệ từ thông thường vs từ thông dụng',
    #caption = 'Chú ý: Không bao gồm các từ thông dụng',
    #fill = 'Loại từ',
    y = 'Tỉ lệ',
    x = 'Loại từ') +
  
  facet_wrap(~source, nrow = 2)+
  
  scale_y_continuous(labels = scales::percent) +
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),          #strip axis ticks
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))+
  
  scale_fill_discrete(name = 'Loại từ',
                      labels = c('Thông thường', 'Thông dụng')) 
```

Số lượng từ vựng thuộc nhóm từ thông dụng (stopwords) chỉ chiếm một phần nhỏ trong bộ từ vựng sử dụng cho bài thi IELTS (khoảng 14%), một con số rất thấp. 
Nếu bạn có một vốn từ vựng ít thì mình nghĩ con số 14% này chính là điểm bạn nên bắt đầu vì đây là những từ rất rất thông dụng.

```{r data-distribution}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  #xlim(c(0, 25)) 
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(
        axis.ticks = element_blank(),          #strip axis ticks
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Mình dùng các histogram để xem xét sự phân bố của dữ liệu. 
Ở đây, mình muốn biết phân phối tần suất xuất hiện của từ vựng trong từng cuốn sách. 
Cần phải nói thêm, trong tập dữ liệu sử dụng cho biểu đồ này, mình đã loại bỏ một số từ thường xuyên xuất hiện trong câu hỏi của đề thi (ví dụ từ questions xuất hiện đến 205 lần).
Việc loại bỏ này được thực hiện là do khi xem xét tần suất xuất hiện của từ vựng theo giá trị từ cao xuống thấp, mình phát hiện một số từ vựng có tần suất xuất hiện cao hơn hẳn các từ khác. 
Để ý thì thấy rằng các từ này đều là những từ hay xuất hiện trong câu hỏi của đề thi. 
Mình coi các từ này là những outlier và loại chúng ra để tránh ảnh hưởng đến tổng quan chung của biểu đồ. 

Khái niệm tần suất (frequency) và số lượng (count) chú thích trên biểu đồ sẽ rất dễ gây nhầm lẫn nên mình muốn giải thích rõ hơn. 
Về giá trị tần suất, ví dụ, nhìn trên biểu đồ, bạn sẽ thấy có một cột rất cao. 
Cột đó có giá trị tần suất là 2. Nghĩa là, trong mỗi cuốn Cambridge, số lượng từ vựng xuất hiện 2 lần trong sách là rất nhiều (có trên 1500 từ như vậy). 
Có rất ít từ xuất hiện trên 25 lần và vì số lượng rất ít nên bạn gần như không thể thấy nó được biểu diễn trên biểu đồ. 
Vậy tại sao mình lại biết là có từ xuất hiện trên 25 lần mặc dù nó không được thể hiện trên biểu đồ? 
Đó là vì, ở trục x, giá trị tần suất được thể hiện đến con số 150. 
Tức là phải có một từ nào đó xuất hiện lớn hơn 150 lần nhưng vì số lượng rất ít nên gần như là không thấy.

Để thể hiện rõ hơn, mình sẽ chia biểu đồ và phóng to vào từng vùng của biều đồ để nhìn số liệu được chính xác hơn. 
Có thể thấy từ biểu đồ chung thì hầu hết các từ vựng đều nằm ở vùng có tần suất nhỏ hơn 25. 
Do vậy, hai vùng được chia bao gồm: từ 0 - 25 và lớn hơn 25.

```{r greater-25}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(25, 200)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu - tần suất từ 25 trở lên',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(
        axis.ticks = element_blank(),          #strip axis ticks
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Trước tiên, chúng ta sẽ xem xét những từ vựng có tuần suất xuất hiện trên 25 lần trong một cuốn sách Cam. 
Cần biết là mỗi cuốn Cam bao gồm 4 test. Xuất hiện 25 lần tức là trung bình mỗi bài test từ sẽ xuất hiện 6 lần. 
Số lượng từ nằm trong nhóm này dù không nhiều nhưng là những từ nhất định phải biết bởi chúng rất thường xuất hiện trong bài thi IELTS. 

```{r less-25}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(0, 24)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu - tần suất dưới 25',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(
        axis.ticks = element_blank(),          #strip axis ticks
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```


Với tần suất dưới 25 lần. Ở đây, các biều đồ phân phối khá tương đồng nhau, chỉ riêng cuốn Cambridge 11 là có chút khác biệt. 
Mục đích của mình khi vẽ biểu đồ phân phối dữ liệu cho từng cuốn riêng biệt là nhằm xác định liệu có sự khác nhau hay không. 
Nếu có khác, mình sẽ phải thực hiện việc lọc dữ liệu cho từng cuốn riêng biệt. 
Nhưng vì sự tương đồng là rất lớn nên mình sẽ gộp chung toàn bộ từ vựng của 6 cuốn và thực hiện việc lọc dựa trên một tập dữ liệu lớn.


```{r distribution-all}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  #facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  #xlim(c(0, 23)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu (tổng từ vựng từ Cambridge 10  - 15)',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  #facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(
        axis.ticks = element_blank(),          #strip axis tick
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Giải thích thêm về biểu đồ.
Mình đã gộp chung từ vựng của tất cả 6 cuốn Cambridge làm một.
Giá trị tần suất các từ trùng nhau của các cuốn sách sẽ được cộng gộp.
Ví dụ, từ 'fun' xuất hiện 1, 4, 7, 3, 8, 2 lần trong lần lượt các cuốn sách từ Cambridge 10 - 15. 
Vậy từ 'fun' đã xuất hiện tổng cộng 25 lần.
Đó cũng là giá trị tần suất sẽ được ghi nhận trên biểu đồ.

Nhìn qua biểu đồ bạn có thể thấy hầu hết từ vựng nằm tập trung ở khoảng tần suất từ 0 đến 50.
Do đó, mình sẽ phóng to vào khoảng này để tìm hiểu thêm.

```{r distribution-all-less-50}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  #facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(0, 50)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu (tổng từ vựng từ Cambridge 10  - 15) - tần suất dưới 50',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  #facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Đến đây thì toang hẳn rồi :v. 
Nếu quay lại các biểu đồ trước, khi phân tích riêng từng cuốn sách, bạn sẽ thấy là tần suất xuất hiện nhiều nhất là 2 lần.
Mình đã hi vọng rằng dù tần suất xuất hiện là 2 nhưng nếu các từ trùng lặp từ cuốn này sang cuốn khác thì vẫn được xem là từ thông dụng.
Nhưng không, bọn nó chỉ xuất hiện đúng 2 lần thật.
Nếu chỉ nhìn bằng mắt thường, chỉ riêng ở giá trị tần suất 1 và 2 đã chiếm tới gần 4000, tức chiếm tới gần 45% tổng số từ vựng (từ vựng trong 6 cuốn sách là 8703 từ, không bao gồm từ thông dụng).
Và nếu giả sử, một từ được coi là có tần suất cao khi nó xuất hiện ít nhất một lần trong một bài test, tức là 24 lần thì từ loại này chỉ chiếm khoảng 10% lượng từ vựng.

Tóm lại là cố gắng trau dồi từ vựng đi các bạn ạ. 
Không ăn gian được đâu :))


```{r words-normal-vs-common}
x <- combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  filter(freq >= 24) %>% 
  summarise(n = sum(freq))

y <- combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  filter(freq < 24) %>% 
  summarise(m = sum(freq))
```

```{r}
value <- c(x[1,1], y[1,1])
word <- c('Tần suất từ 1 trở lên', 'Tần suất dưới 1')
piechart <- data.frame(word, value)

ggplot(piechart, aes(x ="", y = value, fill = word)) +
  geom_bar(width = 1, stat = 'identity')+
  geom_text(aes(label = paste(round(value / sum(value) * 100, 1), "%")),
            position = position_stack(vjust = 0.5)) +
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tỉ lệ từ thông dụng vs từ thông thường') +
  theme_classic()+
  theme(plot.title = element_text(hjust = 0, vjust = 1),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.title = element_blank(),
        axis.title = element_blank())+
  coord_polar("y", start = 0, direction = -1)
```

Tin tốt lành cuối cùng.
Mặc dù chỉ chiếm khoảng 10% nhưng những từ vựng thông dụng chiếm tới 50% tổng lượng từ xuất hiện.

Tổng số từ vựng:  8703  
- freq >= 1: 764
- freq < 1: 7889
- ques_sample: 51
stopwords: 1149

# Kết luận 

“Nội dung của bài thi IELTS phản ánh mọi tình huống, không thiên vị và công bằng đối với tất cả các thí sinh từ mọi hoàn cảnh” (How IELTS is developed). 
Qua việc khai thác dữ liệu chữ từ bộ đề thi thử IELTS chính thức của Cambridge, mình nghĩ hội đồng xây dựng đề thi đã thực hiện rất đúng tiêu chí trên. 
Từ vựng qua các năm được thay đổi rất nhiều để đáp ứng tiêu chí phủ rộng mọi lĩnh vực nhưng vẫn thống nhất về lượng từ xuất hiện để đảm bảo bài thi được xây dựng thống nhất.

# Điểm chưa hoàn chỉnh của bài phân tích

Mình không có nhiều kiến thức về xác suất và thống kê nên mình biết các tiêu chí lọc từ của mình còn rất lôm côm và không có cơ sở toán học để chứng minh. 
Rất mong các bạn độc giả có kiến thức chuyên môn có thể góp ý để hoàn thiện hơn cho bài phân tích này. 

[Link Github](https://github.com/phuongthaost13/IELTS-vocabulary)

[Link từ vựng](https://docs.google.com/spreadsheets/d/1DrZvS7YqJLdIa37VsS67NdaicgWKACrJ0g62hWxMSK0/edit?fbclid=IwAR1iTRohwtjdgEg4xBC5vSLJP4VW22gCNBRRoSgyaLAZC2KV3Y4YqePDE0M#gid=690174637)

