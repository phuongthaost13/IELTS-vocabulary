---
title: "Một số insight về từ vựng sách Cambridge English IELTS"
author: "Phạm Phương Thảo"
date: "8/30/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F, message = F)
knitr::opts_knit$set(root.dir = normalizePath(".."))
```

(Ngắn gọn cho bạn nào lười đọc): File Excel từ vựng tiếng Anh xuất hiện trong sách Cambridge English IELTS từ cuốn 10 - 15. Xem từng ảnh để biết thêm về các tiêu chí lọc từ của mình và cách sử dụng file từ vựng.

```{r library, warning=FALSE, message=FALSE}
library(ggplot2)
library(hrbrthemes)
library(scales)
library(dplyr)
library(data.table)
library(tidytext)
library(purrr) 
#library(plyr) # k load package này bởi vì nó mask dplyr::group_by
```

```{r data-for-vis}
# data import
setwd("~/R project/textMining/ielts-vocab/output/cam10-15")
temp <- list.files(pattern = "*.csv")
myFiles <- lapply(temp, read.csv)


len_myFiles <- length(myFiles)
for (i in 1:len_myFiles) {
  myFiles[[i]]$source <- paste("Cambridge", i+9)
}

# combine all dataframe into one
combined_df <- bind_rows(myFiles)
len_cd <- nrow(combined_df)
for (i in 1:len_cd) {
  if (combined_df$word[i] %in% stop_words$word) {
  combined_df$word_type[i] = 'sw'
} else {
  combined_df$word_type[i] = 'nm'
}
}


# question sample words 
ques_sample <- read.csv(here::here('output', 'ques_sample.csv'))

```

```{r num-words-no-rep}
ggplot(combined_df, aes(x = source)) +
  
  geom_bar(fill = 'light blue', col = 'light blue', alpha = 0.5) +
  
  geom_text(
    stat = 'count',
    aes(label = ..count..),
    vjust = -0.5,
    family = 'serif',
    size = 2.7,
    color = ("dark red"),
    fontface = 'bold')+
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tổng số từ vựng (từ vựng không lặp lại)',
    x = 'Tên sách',
    y = 'Số lượng từ') +
  
  theme_minimal() +
  
  theme(#grid elements
      axis.ticks = element_blank(),          #strip axis ticks
      
      #since theme_minimal() already strips axis lines, 
      #we don't need to do that again
      
      axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
      axis.title.y = element_text(hjust = 1, face = 'bold'))
  
```

Nói một chút về nguồn sách mình sử dụng.
Sách Cambridge English IELTS là sách có bản quyền nên nguồn sách có trên mạng tất cả đều là sách lậu.
Sách này được chia sẻ ở hai dạng: một là PDF chữ và hai là PDF dạng scan hình ảnh.
Trong số 6 cuốn sách mình sử dụng, chỉ duy nhất cuốn Cambridge 11 là ở dạng PDF chữ.
Quá trình chuyển từ PDF chữ thành text để khai thác dữ liệu khá đơn giản và chính xác.
Các cuốn sách còn lại đều phải đưa vào R để thực hiện việc chuyển file hình ảnh thành text, độ chính xác thấp hơn do chất lượng hình ảnh nhiều chỗ không tốt.

Biểu đồ ở ảnh bên cho thấy, số lượng từ vựng xuất hiện trong sách Cambdrige qua các năm xấp xỉ bằng nhau, riêng chỉ cuốn Cambridge 11 có lượng từ lớn hơn rất nhiều.
Có hai lí do để giải thích. 
Một là như mình đã đề cập, cuốn 11 được chuyên từ PDF chữ nên độ chính xác cao hơn.
Hoặc một lí do khác đó là cuốn 11 thực sự có lượng từ lớn hơn các cuốn khác.

```{r num-words-with-rep}
combined_df %>% 
  group_by(source) %>% 
  summarise(n = sum(freq)) %>% 
  
ggplot(aes(x = source, y = n)) +
  
  geom_bar(fill = 'light blue', col = 'light blue', alpha = 0.5, stat = 'identity') +
  
  geom_text(
    stat = 'identity',
    aes(label = n),
    vjust = -0.5,
    family = 'serif',
    size = 2.7,
    color = ("dark red"),
    fontface = 'bold')+
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tổng số từ (từ vựng có lặp lại)',
    caption = 'Có bao gồm các từ thông dụng',
    x = 'Tên sách',
    y = 'Số lượng từ') +
  
  theme_minimal() +
  
  theme(
      axis.ticks = element_blank(),          #strip axis ticks
      
      #since theme_minimal() already strips axis lines, 
      #we don't need to do that again
  
      axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
      axis.title.y = element_text(hjust = 1, face = 'bold'),
      plot.caption = element_text( hjust = 0))             #left align
      
```

Tuy nhiên, khi khai thác ở một khía cạnh khác là tổng số từ (tức các từ vựng có lặp lại) thì lượng từ ở tất cả các cuốn sách đều ngang bằng nhau.
Điều này giúp mình tin rằng cuốn 11 thực sự có lượng từ vựng nhiều hơn các cuốn khác, chứ không phải vì lí do R đã thiếu chính xác trong việc xử lí file hình ảnh thành text.

```{r stopwords-normalwords}
ggplot(combined_df, aes(x = word_type, group = source)) +
  
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = 'count', alpha=0.5)+
  
  geom_text(
    #stat = 'freq',
    aes(label = scales::percent(..prop..),
        y = ..prop..), 
    stat = 'count',
    vjust = 1,
    family = 'serif',
    size = 2.7,
    color = 'black',
    fontface = 'bold') +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tỉ lệ từ thông thường vs từ thông dụng',
    #caption = 'Chú ý: Không bao gồm các từ thông dụng',
    #fill = 'Loại từ',
    y = 'Tỉ lệ',
    x = 'Loại từ') +
  
  facet_wrap(~source, nrow = 2)+
  
  scale_y_continuous(labels = scales::percent) +
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.text.x = element_blank(),
        axis.ticks = element_blank(),          #strip axis ticks
      
      #since theme_minimal() already strips axis lines, 
      #we don't need to do that again
  
      axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
      axis.title.y = element_text(hjust = 1, face = 'bold'),
      plot.caption = element_text( hjust = 0))+
  
  scale_fill_discrete(name = 'Loại từ',
                      labels = c('Thông thường', 'Thông dụng')) 
```

R có tích hợp một tập dữ liệu các từ thông dụng trong tiếng Anh (gọi là stopwords), trong đó bao gồm 1149 từ.
Nếu mình không có chú thích thì tất cả các đồ thị mà mình thực hiện đều đã loại bỏ các từ thông dụng này bởi đây là những từ mà bạn chắc chắn phải học.
Biểu đồ cột bên cho ta thấy, số lượng từ vựng thuộc nhóm từ thông dụng chỉ chiếm một phần nhỏ trong bộ từ vựng sử dụng cho bài thi IELTS (khoảng 14%).
Có 533 từ vựng xuất hiện đồng thời trong cả 6 cuốn Cambridge IELTS (chiếm khoảng 15% tổng số từ).
Nếu cộng cả hai loại trên thì cũng chỉ mới được khoảng 29% số lượng từ vựng của IELTS, một con số rất thấp.

Nếu bạn có một vốn từ vựng ít thì mình nghĩ con số 29% này chính là điểm bạn nên bắt đầu vì đây là những từ rất rất thông dụng.

```{r data-distribution}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  #xlim(c(0, 25)) 
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
      
        #since theme_minimal() already strips axis lines, 
        #we don't need to do that again
  
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Mục đích của mình khi thực hiện việc khai thác dữ liệu chữ từ sách chính là có thể học số lượng tối thiểu từ vựng có khả năng xuất hiện tối đa trong bài thi IELTS.
Hay nói ngắn gọn chính là học những từ có khả năng xuất hiện cao nhất.

Trước tiên, mình vẽ các histogram để xem xét sự phân bố của dữ liệu.
Ở đây, mình muốn biết phân phối tần suất xuất hiện của từ vựng trong từng cuốn sách. 
Cần phải nói thêm, trong tập dữ liệu sử dụng cho biểu đồ này, mình đã loại bỏ một số từ thường xuyên xuất hiện trong câu hỏi của đề thi (ví dụ từ questions xuất hiện đến 205 lần).
Việc loại bỏ này được thực hiện là do khi xem xét tần suất xuất hiện của từ vựng theo giá trị từ cao xuống thấp, mình phát hiện một số từ vựng có tần suất xuất hiện cao hơn hẳn các từ khác.
Để ý thì thấy rằng các từ này đều là những từ hay xuất hiện trong câu hỏi của đề thi.
Mình coi các từ này là những outlier và loại chúng ra để tránh ảnh hưởng đến tổng quan chung của biểu đồ.

Khái niệm tần suất và số lượng mình chú thích trên biểu đồ sẽ rất dễ gây nhầm lẫn nên mình muốn giải thích rõ hơn.
Về giá trị tần suất, ví dụ, nhìn trên biểu đồ, bạn sẽ thấy có một cột rất cao. 
Cột đó có giá trị tần suất là 2.
Nghĩa là, trong mỗi cuốn Cambridge, số lượng từ vựng xuất hiện 2 lần trong sách là rất nhiều (có trên 1500 từ như vậy).
Có rất ít từ xuất hiện trên 25 và vì số lượng rất ít nên bạn gần như không thể thấy nó được biểu diễn trên biểu đồ.
Vậy tại sao mình lại biết là có từ xuất hiện trên 25 lần mặc dù nó không được thể hiện trên biểu đồ?
Đó là vì, ở trục x, giá trị tần suất được thể hiện đến con số 150.
Tức là phải có một từ nào đó xuất hiện lớn hơn 150 lần nhưng vì số lượng rất ít nên gần như là không thấy.

Để thể hiện rõ hơn, mình sẽ chia biểu đồ và phóng to vào từng vùng của biều đồ để nhìn số liệu được chính xác hơn. 
Có thể thấy từ biểu đồ chung thì hầu hết các từ vựng đều nằm ở vùng có tần suất nhỏ hơn 25.
Do vậy, hai vùng được chia bao gồm: từ 0 - 25 và lớn hơn 25.

```{r greater-25}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(25, 200)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu - tần suất từ 25 trở lên',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
      
        #since theme_minimal() already strips axis lines, 
        #we don't need to do that again
  
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Trước tiên, chúng ta sẽ xem xét những từ vựng có tuần suất xuất hiện trên 25 lần trong một cuốn sách Cambridge.
Cần biết là mỗi cuốn Cambridge bao gồm 4 test.
Xuất hiện 25 lần tức là trung bình mỗi bài test từ sẽ xuất hiện 6 lần.
Số lượng từ nằm trong nhóm này dù không nhiều nhưng là những từ nhất định phải biết bởi chúng rất thường xuất hiện trong bài thi IELTS.
Bạn cũng có thể thấy là số lượng từ và phân phối của từ giữa các cuốn sách rất tương đồng nhau (ở cả biểu đồ này và những biểu đồ trước đó).
Điều này cho thấy, sách thực sự được làm rất chất lượng.

```{r less-25}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  group_by(source) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(0, 24)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu - tần suất dưới 25',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
      
        #since theme_minimal() already strips axis lines, 
        #we don't need to do that again
  
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```


Ở đây, các biều đồ phân phối khá tương đồng nhau, chỉ riêng cuốn Cambridge 11 là có chút khác biệt.
Mục đích của mình khi vẽ biểu đồ phân phối dữ liệu cho từng cuốn riêng biệt là nhằm xác định liệu có sự khác nhau hay không.
Nếu có khác, mình sẽ phải thực hiện việc lọc dữ liệu cho từng cuốn riêng biệt.
Nhưng vì sự tương đồng là rất lớn nên mình sẽ gộp chung toàn bộ từ vựng của 6 cuốn và thực hiện việc lọc dựa trên một tập dữ liệu lớn.


```{r distribution-all}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  #facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  #xlim(c(0, 23)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu (tổng từ vựng từ Cambridge 10  - 15)',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  #facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
      
        #since theme_minimal() already strips axis lines, 
        #we don't need to do that again
  
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Giải thích thêm về biểu đồ.
Mình đã gộp chung từ vựng của tất cả 6 cuốn Cambridge làm một.
Giá trị tần suất các từ trùng nhau của các cuốn sách sẽ được cộng gộp.
Ví dụ, từ 'fun' xuất hiện 1, 4, 7, 3, 8, 2 lần trong lần lượt các cuốn sách từ Cambridge 10 - 15. 
Vậy từ 'fun' đã xuất hiện tổng cộng 25 lần.
Đó cũng là giá trị tần suất sẽ được ghi nhận trên biểu đồ.

Nhìn qua biểu đồ bạn có thể thấy hầu hết từ vựng nằm tập trung ở khoảng tần suất từ 0 đến 50.
Do đó, mình sẽ phóng to vào khoảng này để tìm hiểu thêm.

```{r distribution-all-less-50}
combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  ggplot(aes(x = freq)) +
  geom_histogram(binwidth = 1)+
  #facet_wrap(~source, nrow = 3)+
  theme_minimal()+
  xlim(c(0, 50)) +
  
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Phân phối của dữ liệu (tổng từ vựng từ Cambridge 10  - 15) - tần suất dưới 50',
    caption = 'bin width = 1',
    y = 'Số lượng (lần)',
    x = 'Tần suất (lần)') +
  
  #facet_wrap(~source, nrow = 3)+
  
  theme_minimal() +
  
  theme(legend.position = 'bottom',
        legend.title = element_text(face = 'bold'),
        axis.ticks = element_blank(),          #strip axis ticks
      
        #since theme_minimal() already strips axis lines, 
        #we don't need to do that again
  
        axis.title.x = element_text(hjust = 1, vjust = 0, face = 'bold'),
        axis.title.y = element_text(hjust = 1, face = 'bold'),
        plot.caption = element_text( hjust = 0))             #left align
```

Đến đây thì toang hẳn rồi :v. 
Nếu quay lại các biểu đồ trước, khi phân tích riêng từng cuốn sách, bạn sẽ thấy là tần suất xuất hiện nhiều nhất là 2 lần.
Mình đã hi vọng rằng dù tần suất xuất hiện là 2 nhưng nếu các từ trùng lặp từ cuốn này sang cuốn khác thì vẫn được xem là từ thông dụng.
Nhưng không, bọn nó chỉ xuất hiện đúng 2 lần thật.
Nếu chỉ nhìn bằng mắt thường, chỉ riêng ở giá trị tần suất 1 và 2 đã chiếm tới gần 4000, tức chiếm tới gần 45% tổng số từ vựng (từ vựng trong 6 cuốn sách là 8703 từ, không bao gồm từ thông dụng).
Và nếu giả sử, một từ được coi là thông dụng khi nó xuất hiện ít nhất một lần trong một bài test, tức là 24 lần trong 6 cuốn sách thì lượng từ vựng có tần suất nhỏ hơn 24 chiếm khoảng 90% tổng lượng từ :((

Tóm lại là cố gắng trau dồi từ vựng đi các bạn ạ. 
Không ăn gian được đâu :))


```{r words-normal-vs-common}
x <- combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  filter(freq >= 24) %>% 
  summarise(n = sum(freq))

y <- combined_df %>% 
  filter(!(word %in% stop_words$word)) %>%
  filter(!(word %in% ques_sample$word)) %>% 
  plyr::ddply("word", plyr::numcolwise(sum)) %>% 
  filter(freq < 24) %>% 
  summarise(m = sum(freq))
```

```{r}
value <- c(x[1,1], y[1,1])
word <- c('Tần suất từ 1 trở lên', 'Tần suất dưới 1')
piechart <- data.frame(word, value)

ggplot(piechart, aes(x ="", y = value, fill = word)) +
  geom_bar(width = 1, stat = 'identity')+
  geom_text(aes(label = paste(round(value / sum(value) * 100, 1), "%")),
            position = position_stack(vjust = 0.5)) +
  labs(
    title = 'Từ vựng sách Cambridge English IELTS 10 - 15',
    subtitle = 'Tỉ lệ từ thông dụng vs từ thông thường') +
  theme_classic()+
  theme(plot.title = element_text(hjust = 0, vjust = 1),
        axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        legend.title = element_blank(),
        axis.title = element_blank())+
  coord_polar("y", start = 0, direction = -1)
```

Tin tốt lành cuối cùng.
Mặc dù chỉ chiếm khoảng 10% nhưng những từ vựng thông dụng chiếm tới 50% tổng lượng từ xuất hiện.

Mặc dù kết quả không như mong đợi nhưng dù gì cũng đã chạy đông chạy tây được đến đây nên mình vẫn tạo một bảng từ vựng các từ xuất hiện trong sách Cambridge English IELTS 10 - 15.
Hi vọng nó sẽ có ích nếu bạn cần một bảng từ vựng để sử dụng cho Anki chẳng hạn.

File từ vựng sẽ bao gồm:

- stopwords: 1149 từ vựng thông dụng trong tiếng Anh

- greater than 1: các từ vựng có tần suất xuất hiện trung bình từ 1 lần trở lên trong mỗi cuốn sách

- less than 1: các từ vựng có tần suất xuất hiện trung bình dưới 1 lần trong mỗi cuốn sách

Tổng số từ vựng:  8703  
- freq >= 1: 764
- freq < 1: 7889
- ques_sample: 51
stopwords: 1149

